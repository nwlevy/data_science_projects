{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Solar Panel Adoption - Models Trained with Dataset Variations\n",
    "#### UC Berkeley MIDS\n",
    "`Team: Gabriel Hudson, Noah Levy, Laura Williams`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains our final model on dataset variations for comparison to our final dataset and model. For more details or explanations about the choices we made here, see our other work in the _Modeling_ folder in this repo.\n",
    "\n",
    "**Dataset**  \n",
    "See the _Dataset Variations_ notebook in this _Modeling Folder_ for more information about the datasets used in this notebook.  \n",
    "\n",
    "This notebook is set up to load the uncompressed dataset stored in a separate _Datasets_ folder that was not uploaded to the repo due to the large size of the uncompressed dataset.\n",
    "\n",
    "**Model**   \n",
    "In this notebok we train two Random Forest sequential models:\n",
    "* Random Forest Classifier to predict presence or absence of residential solar panels  \n",
    "* Random Forest Regressor to predict residential solar panel density and analyize most important predictive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original raw dataset and idea for the two-stage Random Forest model came from Stanford's DeepSolar team, who recently completed the most accurate detection to date of all solar panels in the US, using satellite imagery and a convolutional neural network:  \n",
    "http://web.stanford.edu/group/deepsolar/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import rfpimp as rfp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if necessary, install package on your local machine for calculating feature importances\n",
    "#!pip install rfpimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load curated dataset for final model\n",
    "# deepsolar = pd.read_csv('../Datasets/SolarPrediction_Final_Dataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VARIATION\n",
    "# load curated dataset with original outcome variable\n",
    "deepsolar = pd.read_csv('../Datasets/Final_OldOutcomeVar.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VARIATION\n",
    "# load curated dataset without latitude or longitude\n",
    "deepsolar = pd.read_csv('../Datasets/Final_NoLatLon.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset rows and dimensions: (71305, 58)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset rows and dimensions:\", deepsolar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define outcome and featureset variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VARIATION\n",
    "# set variable for stage 2 Regressor for dataset variation with original outcome variable\n",
    "# outcome_var_R = 'number_of_solar_system_per_household'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set variable for stage 2 Regressor for final model\n",
    "outcome_var_R = 'owner_occupied_solar_system_density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create binary outcome variable for stage 1 random forest Classifier\n",
    "deepsolar['solar_flag']=deepsolar[outcome_var_R].apply(lambda x: int(x>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New binary outcome variable for Stage 1 random forest classifier 'solar_flag' has 2 values: 0 and 1\n",
      "New dataset rows and dimensions: (71305, 59)\n"
     ]
    }
   ],
   "source": [
    "# Confirm values in new outcome variable\n",
    "print(\"New binary outcome variable for Stage 1 random forest classifier 'solar_flag' has\", \n",
    "      deepsolar['solar_flag'].nunique(), \"values:\", deepsolar[\"solar_flag\"].min(),\n",
    "      \"and\", deepsolar[\"solar_flag\"].max())\n",
    "print(\"New dataset rows and dimensions:\", deepsolar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set variable for stage 1 Classifier\n",
    "outcome_var_C = 'solar_flag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 57\n"
     ]
    }
   ],
   "source": [
    "# set variable for feature names\n",
    "feature_cols = deepsolar.drop(labels=[outcome_var_C, outcome_var_R], axis=1).columns.values\n",
    "print(\"Number of features:\", len(feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split outcome variables from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full featureset shape is (71305, 57)\n",
      "Classifier outcome variable shape: (71305,)\n",
      "Regressor outcome variable shape: (71305,)\n"
     ]
    }
   ],
   "source": [
    "# separate outcome variables and features\n",
    "X = deepsolar.drop(labels=[outcome_var_C, outcome_var_R], axis=1).values\n",
    "Y_classifier = deepsolar[outcome_var_C].values\n",
    "Y_regressor = deepsolar[outcome_var_R].values\n",
    "print(\"Full featureset shape is\", X.shape)\n",
    "print(\"Classifier outcome variable shape:\", Y_classifier.shape)\n",
    "print(\"Regressor outcome variable shape:\", Y_regressor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 10-fold cross-validation to train our model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_splits(X, Y_classifier, Y_regressor, n_splits):\n",
    "    \"\"\"Create dataset splits for cross-validated hyperparameter tuning.\n",
    "    \n",
    "    Input:  dataset features(X), two outcome variables(Y_classifier and Y_regressor), \n",
    "            number of splits.\n",
    "    Output: train and test set as lists of splits\n",
    "    \"\"\"\n",
    "    # set up splits\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=None)\n",
    "    # initialize variables\n",
    "    X_trains=[]\n",
    "    X_tests=[]\n",
    "    YC_trains=[]\n",
    "    YC_tests=[]\n",
    "    YR_trains = []\n",
    "    YR_tests = []\n",
    "    # create lists of data splits\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_trains.append(X[train_index])\n",
    "        X_tests.append(X[test_index])\n",
    "        YC_trains.append(Y_classifier[train_index])\n",
    "        YC_tests.append(Y_classifier[test_index])\n",
    "        YR_trains.append(Y_regressor[train_index])\n",
    "        YR_tests.append(Y_regressor[test_index])\n",
    "    return X_trains, X_tests, YC_trains, YC_tests, YR_trains, YR_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_2stage_model_CV(n_splits, \n",
    "                          outcome_var_C, outcome_var_R, feature_cols,\n",
    "                          n_C, depth_C, features_C, \n",
    "                          n_R, depth_R, features_R,\n",
    "                          X_trains, X_tests,\n",
    "                          YC_trains, YC_tests,\n",
    "                          YR_trains, YR_tests):\n",
    "    \"\"\"Two-stage Random Forest model is trained using cross-validation.    \n",
    "    Stage 1: Classifier predicting residential solar panel presence or absence in a census tract\n",
    "    Stage 2: Regressor predicting residential solar panel density\n",
    "    \n",
    "    Input: hyperparameters for Stage 1 classifier, hyperparameters for Stage 2 regressor,\n",
    "    dataset splits as created above\n",
    "    Output: classifer, regressor, combined R squared scores and feature importances for each datasplit\n",
    "    \"\"\"\n",
    "    \n",
    "    def important_features(model, feature_cols, outcome_var, X_test, Y_test):\n",
    "        \"\"\"Calculate important features from each model from within cross-validation model training\"\"\"\n",
    "        importances_permutation = rfp.importances(model, \n",
    "                                        pd.DataFrame(X_test,columns=feature_cols), \n",
    "                                        pd.DataFrame(Y_test,columns=[outcome_var]))\n",
    "        importances_standard = model.feature_importances_\n",
    "        return importances_permutation, importances_standard\n",
    "    \n",
    "    # initialize variables\n",
    "    classifier_scores = []\n",
    "    regressor_scores = []\n",
    "    combined_scores = []\n",
    "    feature_importances_RP = []\n",
    "    feature_importances_RS = []\n",
    "    feature_importances_CP = []\n",
    "    feature_importances_CS = []\n",
    "    # train model and record output for each model\n",
    "    for i in range(n_splits):\n",
    "        # define and fit classifier\n",
    "        classifier = RandomForestClassifier(n_estimators=n_C, max_depth=depth_C, max_features=features_C, n_jobs=-1)\n",
    "        classifier.fit(X_trains[i], YC_trains[i])  \n",
    "        # define and fit regressor\n",
    "        regressor = RandomForestRegressor(n_estimators=n_R, max_depth=depth_R, max_features=features_R, n_jobs=-1)\n",
    "        regressor.fit(X_trains[i], YR_trains[i])\n",
    "        # calculate individual scores\n",
    "        classifier_score = classifier.score(X_tests[i],YC_tests[i])\n",
    "        regressor_score = regressor.score(X_tests[i],YR_tests[i])\n",
    "        # calculate combined model scores\n",
    "        classifier_preds=classifier.predict(X_tests[i])\n",
    "        regressor_preds=regressor.predict(X_tests[i])\n",
    "        final_preds=regressor_preds*classifier_preds\n",
    "        combined_score = r2_score(YR_tests[i],final_preds)\n",
    "        # add scores to lists for output\n",
    "        classifier_scores.append(classifier_score)\n",
    "        regressor_scores.append(regressor_score)\n",
    "        combined_scores.append(combined_score)\n",
    "        # calculate feature importances and add to lists for output\n",
    "        importances_CP, importances_CS = important_features(classifier, feature_cols, outcome_var_C, \n",
    "                                                            X_tests[i], YC_tests[i])\n",
    "        feature_importances_CP.append(importances_CP)\n",
    "        feature_importances_CS.append(importances_CS) \n",
    "        importances_RP, importances_RS = important_features(regressor, feature_cols, outcome_var_R, \n",
    "                                                            X_tests[i], YR_tests[i])\n",
    "        feature_importances_RP.append(importances_RP)\n",
    "        feature_importances_RS.append(importances_RS)\n",
    "       \n",
    "        print(\"Finished iteration\", i+1)\n",
    "        \n",
    "    return classifier_scores, regressor_scores, combined_scores, \\\n",
    "           feature_importances_RP, feature_importances_RS, feature_importances_CP, feature_importances_CS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set number of splits for cross-validation\n",
    "n_splits = 10\n",
    "# create splits\n",
    "X_trains, X_tests, \\\n",
    "YC_trains, YC_tests, \\\n",
    "YR_trains, YR_tests = create_splits(X, Y_classifier, Y_regressor, n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters for classifier\n",
    "depth_C = 25\n",
    "features_C = 0.25\n",
    "n_C = 150\n",
    "# Set hyperparameters for classifier\n",
    "depth_R = 25\n",
    "features_R = 0.35\n",
    "n_R = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished iteration 1\n",
      "Finished iteration 2\n",
      "Finished iteration 3\n",
      "Finished iteration 4\n",
      "Finished iteration 5\n",
      "Finished iteration 6\n",
      "Finished iteration 7\n",
      "Finished iteration 8\n",
      "Finished iteration 9\n",
      "Finished iteration 10\n",
      "Processing time: 22.56 minutes\n"
     ]
    }
   ],
   "source": [
    "# train the model with cross-validation\n",
    "start = time.time()\n",
    "classifier_scores, \\\n",
    "regressor_scores, \\\n",
    "combined_scores, \\\n",
    "feature_importances_RP, \\\n",
    "feature_importances_RS, \\\n",
    "feature_importances_CP, \\\n",
    "feature_importances_CS = train_2stage_model_CV(n_splits, \n",
    "                                               outcome_var_C, outcome_var_R, feature_cols,\n",
    "                                               n_C, depth_C, features_C, \n",
    "                                               n_R, depth_R, features_R,\n",
    "                                               X_trains, X_tests,\n",
    "                                               YC_trains, YC_tests,\n",
    "                                               YR_trains, YR_tests)\n",
    "print(\"Processing time: {} minutes\".format(round((time.time() - start)/60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first set of results is from the dataset with the original outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average classifier R squared is 0.812916342391\n",
      "Average regressor R squared is 0.714364287294\n",
      "Average combined R squared is 0.716094604403\n"
     ]
    }
   ],
   "source": [
    "print(\"Average classifier R squared is\", np.array(classifier_scores).mean())\n",
    "print(\"Average regressor R squared is\", np.array(regressor_scores).mean())\n",
    "print(\"Average combined R squared is\", np.array(combined_scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second set of results is from the final dataset (with owner-occupied modified outcome variable) with latitude and longitude removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average classifier R squared is 0.811317543082\n",
      "Average regressor R squared is 0.72013099663\n",
      "Average combined R squared is 0.721189813084\n"
     ]
    }
   ],
   "source": [
    "print(\"Average classifier R squared is\", np.array(classifier_scores).mean())\n",
    "print(\"Average regressor R squared is\", np.array(regressor_scores).mean())\n",
    "print(\"Average combined R squared is\", np.array(combined_scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the output regarding important features below is from the final model, not the dataset variations.  Code is retained in this notebook in the event anyone wants to later look at feature importances from the dataset variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regressor permutation importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine average feature importances from each split in the cross-validation\n",
    "combined_RP = pd.concat(feature_importances_RP, axis=1, join='inner')\n",
    "# confirm shape\n",
    "# print(combined_RP.shape)\n",
    "# calculate mean and sort\n",
    "feature_importances_regressor_permutation = combined_RP.mean(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor top 10 feature importances calculated with permutation importance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Feature\n",
       "incentive_count_residential        0.174510\n",
       "median_household_income            0.146967\n",
       "household_type_family_rate         0.093561\n",
       "lon                                0.065650\n",
       "daily_solar_radiation              0.059953\n",
       "population_density                 0.053259\n",
       "voting_2016_dem_percentage         0.050848\n",
       "relative_humidity                  0.046750\n",
       "electricity_consume_residential    0.043912\n",
       "lat                                0.026844\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Regressor top 10 feature importances calculated with permutation importance:\")\n",
    "feature_importances_regressor_permutation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save permutation importances to file\n",
    "feature_importances_regressor_permutation.to_csv(\"FeatureImportances/importances_regressor.csv\",\n",
    "                                                 header=[\"Importance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if needed, to reload feature importances from CSV\n",
    "feature_importances_R_from_file = pd.read_csv('FeatureImportances/importances_regressor.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressor standard gini importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of numpy arrays to pandas dataframes, adding feature names\n",
    "feature_importances_RS_df = []\n",
    "for i in range(len(feature_importances_RS)):\n",
    "    df = pd.DataFrame(feature_importances_RS[i], index=feature_cols, columns=[\"Importance\"])\n",
    "    feature_importances_RS_df.append(df)\n",
    "# combine average feature importances from each split in the cross-validation\n",
    "combined_RS = pd.concat(feature_importances_RS_df, axis=1, join='inner')\n",
    "# calculate mean and sort\n",
    "feature_importances_regressor_standard = combined_RS.mean(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor top 10 feature importances calculated with standard gini importance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "median_household_income            0.100206\n",
       "incentive_count_residential        0.073450\n",
       "household_type_family_rate         0.071812\n",
       "population_density                 0.055634\n",
       "daily_solar_radiation              0.053126\n",
       "housing_unit_median_gross_rent     0.045929\n",
       "lon                                0.041750\n",
       "electricity_consume_residential    0.038059\n",
       "relative_humidity                  0.035466\n",
       "health_insurance_none_rate         0.027690\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Regressor top 10 feature importances calculated with standard gini importance:\")\n",
    "feature_importances_regressor_standard[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier permutation importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine average feature importances from each split in the cross-validation\n",
    "combined_CP = pd.concat(feature_importances_CP, axis=1, join='inner')\n",
    "# calculate mean and sort\n",
    "feature_importances_classifier_permutation = combined_CP.mean(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier top 10 feature importances calculated with permutation importance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Feature\n",
       "population_density             0.04614\n",
       "heating_fuel_coal_coke_rate    0.00796\n",
       "lon                            0.00560\n",
       "race_black_africa_rate         0.00308\n",
       "sales_tax                      0.00188\n",
       "education_master_rate          0.00180\n",
       "daily_solar_radiation          0.00174\n",
       "relative_humidity              0.00156\n",
       "travel_time_10_19_rate         0.00156\n",
       "occupancy_vacant_rate          0.00146\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Classifier top 10 feature importances calculated with permutation importance:\")\n",
    "feature_importances_classifier_permutation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save permutation importances to file\n",
    "feature_importances_classifier_permutation.to_csv(\"FeatureImportances/importances_classifier.csv\",\n",
    "                                                  header=[\"Importance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if needed, to reload feature importances from CSV\n",
    "feature_importances_C_from_file = pd.read_csv('FeatureImportances/importances_classifier.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier standard gini importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of numpy arrays to pandas dataframes, adding feature names\n",
    "feature_importances_CS_df = []\n",
    "for i in range(len(feature_importances_CS)):\n",
    "    df = pd.DataFrame(feature_importances_CS[i], index=feature_cols, columns=[\"Importance\"])\n",
    "    feature_importances_CS_df.append(df)\n",
    "# combine average feature importances from each split in the cross-validation\n",
    "combined_CS = pd.concat(feature_importances_CS_df, axis=1, join='inner')\n",
    "# calculate mean and sort\n",
    "feature_importances_classifier_standard = combined_CS.mean(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier top 10 feature importances calculated with standard gini importance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "population_density                     0.112130\n",
       "heating_fuel_coal_coke_rate            0.039757\n",
       "lon                                    0.036234\n",
       "housing_unit_median_gross_rent         0.030346\n",
       "occupancy_vacant_rate                  0.029547\n",
       "race_black_africa_rate                 0.023555\n",
       "per_capita_income                      0.023194\n",
       "education_high_school_graduate_rate    0.023013\n",
       "education_bachelor_rate                0.022209\n",
       "relative_humidity                      0.021737\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Classifier top 10 feature importances calculated with standard gini importance:\")\n",
    "feature_importances_classifier_standard[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create cross-validated predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The predictions are created with models trained on the cross-validated splits created by sklearn's `cross_val_predict` function instead of the cross-validated splits in the function above.  However, the same hyperparameters are used to train the model and we expect the results to be similar without any significant differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the classifier\n",
    "classifier_for_preds = RandomForestClassifier(n_estimators=n_C, \n",
    "                                              max_depth=depth_C, \n",
    "                                              max_features=features_C, \n",
    "                                              n_jobs=-1)\n",
    "# define the regressor\n",
    "regressor_for_preds = RandomForestRegressor(n_estimators=n_R, \n",
    "                                            max_depth=depth_R, \n",
    "                                            max_features=features_R, \n",
    "                                            n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create cross-validated predictions for the classifier\n",
    "classifier_preds = cross_val_predict(classifier_for_preds, X, Y_classifier, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create cross-validated predictions for the regressor\n",
    "regressor_preds = cross_val_predict(regressor_for_preds, X, Y_regressor, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create combined final predictions\n",
    "final_predictions = classifier_preds*regressor_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect predictions with census tract FIPS code\n",
    "final_preds_fips = pd.DataFrame(final_predictions, index=deepsolar.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27145011200</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27145011301</th>\n",
       "      <td>0.001050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27145011302</th>\n",
       "      <td>0.000811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27145011304</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27145011400</th>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "27145011200  0.000000\n",
       "27145011301  0.001050\n",
       "27145011302  0.000811\n",
       "27145011304  0.000000\n",
       "27145011400  0.000763"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds_fips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to CSV\n",
    "final_preds_fips.to_csv(\"Final_Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to load final predictions from CSV\n",
    "final_preds_fips_from_file = pd.read_csv(\"Final_Predictions.csv\", index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
